{"cells":[{"cell_type":"markdown","source":["# **Build probability matrices from word counts**"],"metadata":{"id":"6wIcop_vDNlH"},"id":"6wIcop_vDNlH"},{"cell_type":"code","execution_count":null,"id":"495aff3f","metadata":{"id":"495aff3f"},"outputs":[],"source":["def clean_Mwd_matrix(Mwd,words,documents):\n","\n","  # remove elements that are too central, e.g., #covid19\n","  not_wanted = np.array(Mwd.sum(axis=1)).flatten()>Mwd.shape[1]/4\n","  text = \"removing: \" + \" \".join(words[not_wanted])\n","  words = words[~not_wanted]\n","  Mwd = Mwd[~not_wanted,:]\n","\n","  # remove documents and words with fewer than 2 links\n","  while True:\n","\n","    # keep memory\n","    dim_old = Mwd.size\n","    # remove documents with less than 2 words\n","    wanted = np.array(Mwd.sum(axis=0)).flatten()>1\n","    Mwd = Mwd[:,wanted]\n","    documents = documents[wanted]\n","\n","    # remove words in less than 2 documents\n","    not_wanted = np.array(Mwd.sum(axis=1)).flatten()<=1\n","    text = text + \" \" + \" \".join(words[not_wanted])\n","    words = words[~not_wanted]\n","    Mwd = Mwd[~not_wanted,:]\n","    # exit criterion\n","    if (dim_old == Mwd.size): break\n","\n","  # exit\n","  print(text)\n","  return Mwd, words, documents\n","\n","def logg(x):\n","    y = np.log(x)\n","    y[x==0] = 0\n","    return y\n","\n","def probability_matrices(Mwd, equalik = True, tform = False):\n","\n","    if equalik: # documents equally likely\n","        Pwd = Mwd/Mwd.sum(axis=0).flatten()/Mwd.shape[1]\n","    else: # documents proportional to their length\n","        Pwd = Mwd/Mwd.sum()\n","    # TF-IDF format\n","    if (tform):\n","        iw = -logg(np.sum(Mwd>0,axis=1).flatten()/Mwd.shape[1])\n","        Pwd = sps.diags(np.array(iw)[0])*Pwd # TF-IDF form\n","        Pwd = Pwd/Pwd.sum() # normalize, treat it as Pwd\n","    # words and document matrices\n","    pd = Pwd.sum(axis=0).flatten()\n","    Pww = (Pwd/pd)*(Pwd.T)\n","    pw = Pwd.sum(axis=1).flatten()\n","    Pdd = (Pwd.T/pw)*Pwd\n","    # joint words and document matrix - documents first\n","    Paa = sps.hstack((sps.csr_matrix((Pwd.shape[1],Pwd.shape[1])),Pwd.T))\n","    Paa = sps.vstack((Paa,sps.hstack((Pwd,sps.csr_matrix((Pwd.shape[0],Pwd.shape[0]))))))\n","    Paa = Paa/2.0\n","\n","    return Pwd, Pww, Pdd, Paa"]},{"cell_type":"markdown","source":["# **Define performance measures**"],"metadata":{"id":"WUgPXJaeSQr2"},"id":"WUgPXJaeSQr2"},{"cell_type":"code","source":["def nmi_function(A): # A = Pwc\n","    aw = A.sum(axis=1).flatten() # word probability\n","    ac = A.sum(axis=0).flatten() # class probability\n","    Hc = np.multiply(ac,-logg(ac)).sum() # class entropy\n","    A2 = ((A/ac).T/aw).T\n","    A2.data = logg(A2.data)\n","    y = (A.multiply(A2)).sum()/Hc\n","    return y\n","\n","def modularity_function(A):\n","    y = A.trace()-(A.sum(axis=0)*A.sum(axis=1)).item()\n","    return y\n","\n","def ncut_function(A):\n","    y = ((A.sum(axis=0)-A.diagonal())/A.sum(axis=0)).mean()\n","    return y\n","\n","def my_pagerank(M,q,c=.85,it=60):\n","    r = q.copy() # ranking matrix, initialized to q (copy)\n","    for k in range(it): # slow cycle\n","      r = c*M.dot(r) + (1-c)*q\n","    return r\n","\n","def infomap_function(v):\n","    y = -(v.data*logg(v.data/v.sum())).sum()\n","    return y\n","\n","def infomap_rank(Pdd):\n","    # transition matrix\n","    pd = Pdd.sum(axis=0).flatten()\n","    M = sps.csr_matrix(Pdd/pd)\n","    # pagerank vector - faster than r = my_pagerank(M,q)\n","    G = ig.Graph.Adjacency((M > 0).toarray().tolist())\n","    G.es['weight'] = np.array(M[M.nonzero()])[0]\n","    r = G.pagerank(weights='weight')\n","    r = (sps.csr_matrix(np.array(r))).T\n","\n","    return r\n","\n","def infomap(C,Pdd,r):\n","    pd = Pdd.sum(axis=0).flatten()\n","    M = Pdd/pd # transition matrix\n","    # extract vectors\n","    z = C.T*sps.diags(r.toarray().flatten())\n","    q = sps.csr_matrix((1,z.shape[0]))\n","    c = .85\n","    for i in range(z.shape[0]):\n","      tmp = (C[:,i].transpose()*M)*z[i].transpose()\n","      q[0,i] = (1-(1-c)*C[:,i].sum()/M.shape[0])*z[i].sum()-c*tmp[0,0]\n","    # extract statistics\n","    y = infomap_function(q)\n","    for i in range(z.shape[0]):\n","      y += infomap_function(sps.hstack([z[i],sps.csr_matrix([[q[0,i]]])]))\n","    # normalize\n","    y = (y/infomap_function(pd))-1\n","\n","    return y\n","\n","def clustering_statistics(C,Pwd,Pdd,r):\n","\n","    Pwc = Pwd*C # joint word + class probability\n","    NMI = nmi_function(Pwc)\n","    Pcc = C.T*Pdd*C # joint class + class probability\n","    Q = modularity_function(Pcc)\n","    Ncut = ncut_function(Pcc)\n","    Infomap = infomap(C,Pdd,r)\n","\n","    return [NMI, Q, Ncut, Infomap]"],"metadata":{"id":"Lu3flGoeSW3B"},"id":"Lu3flGoeSW3B","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **PLMP**"],"metadata":{"id":"Np39LQvZ3Lze"},"id":"Np39LQvZ3Lze"},{"cell_type":"code","source":["def plmp(Mdw,v):\n","  B1 = (Mdw/Mdw.sum(axis=0).flatten()).T\n","  B2 = (Mdw.T/Mdw.sum(axis=1).flatten()).T\n","  M = B1*B2 # mixing matrix\n","  q = B1*v # teleport vector\n","  r = my_pagerank(M,q)\n","  return r"],"metadata":{"id":"evE9CHPu3KvA"},"id":"evE9CHPu3KvA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **BERTopic**"],"metadata":{"id":"wLMY9NGdaao5"},"id":"wLMY9NGdaao5"},{"cell_type":"code","source":["import copy\n","\n","def topics_to_C(topics):\n","\n","  # extract community assignments\n","  C = sps.csr_matrix((len(topics),max(topics)+2))\n","  for i in range(C.shape[1]):\n","    C[np.array(topics)==(i-1),i] = 1\n","\n","  # remove zero assignments\n","  C = C[:,np.unique(scipy.sparse.find(C)[1])]\n","\n","  return C\n","\n","def plot_community_patterns(C,nrows,ncols,refs):\n","\n","  fig, axs = plt.subplots(nrows=nrows, ncols=ncols,\n","                          figsize=(20*nrows, 20*ncols),\n","                          subplot_kw={'xticks': [], 'yticks': []})\n","\n","  for i in range(len(C)):\n","    # identify an order for rankings based on refs\n","    if ((i%ncols)==0):\n","      tmp0 = C[refs[0]+i]\n","      tmp0 = np.array([tmp0[i].argmax()+1 for i in range(tmp0.shape[0])])\n","      tmp1 = C[refs[1]+i]\n","      tmp1 = np.array([tmp1[i].argmax()+1 for i in range(tmp1.shape[0])])\n","      pos = np.argsort(tmp0+(tmp1/tmp1.max())/2)\n","\n","    # plot matrices\n","    tmp = sps.csr_matrix(C[i]).astype(np.float32)\n","    tmp = tmp[pos,] # reorder\n","    M = tmp*(tmp.T)\n","    ax = axs.flat[i]\n","    ax.imshow(M.toarray(), cmap='viridis')\n","\n","\n","print('bertopic 1.11')\n","\n","def bertopic_overwrite(bert_model_in,docs,C):\n","  bert_model = copy.deepcopy(bert_model_in)\n","\n","  # build the documents dataframe: 'Document' + \"Topic\"\n","  documents = pd.DataFrame(docs,columns=['Document'])\n","  tmp = np.array([C[i].argmax() for i in range(C.shape[0])])\n","  documents[\"Topic\"] = tmp\n","\n","  # update topic assignment\n","  bert_model.topics_ = tmp.tolist()\n","\n","  # build cf-idf values\n","  documents_per_topic = documents.groupby(['Topic'],\n","                    as_index=False).agg({'Document': ' '.join})\n","  c_tf_idf_, words = bert_model._c_tf_idf(documents_per_topic)\n","  bert_model.c_tf_idf_ = c_tf_idf_\n","\n","  # extract words representations\n","  topic_representations_ = bert_model._extract_words_per_topic(words, documents)\n","  bert_model.topic_representations_ = topic_representations_\n","  bert_model.topic_labels_ = {key: f\"{key}_\" + \"_\".join([word[0] for word in values[:4]])\n","                              for key, values in\n","                              topic_representations_.items()}\n","\n","  # exit\n","  return bert_model"],"metadata":{"id":"jFY2zC0LadxK"},"id":"jFY2zC0LadxK","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **soft Louvain algorithm**"],"metadata":{"id":"MX16at3BDT2r"},"id":"MX16at3BDT2r"},{"cell_type":"code","source":["def soft_assign(a,v):\n","  if (a>=0):\n","    u = np.array(v.data)\n","    n = np.where(u==u.max())[0][0]\n","    u = np.zeros(u.shape)\n","    u[n] = 1\n","    return np.array(u)/u.sum()\n","  else:\n","    u = -np.array(v.data)/a\n","    g = np.sort(u)[::-1]\n","    z = np.cumsum(g)-np.append(np.array(range(1,len(g)))*g[1:len(g)],-np.Inf)\n","    n = np.where(z>=1)[0][0]\n","    la = ((g[0:n+1]).sum()-1)/(n+1)\n","    u = u-la\n","    u[u<0] = 0\n","    return np.array(u)/u.sum()"],"metadata":{"id":"ylPXUvEEv_cP"},"id":"ylPXUvEEv_cP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["from random import shuffle\n","\n","def my_soft_louvain(A, C_start=None, seed=None):\n","    \"\"\"\n","    Find the best partition of a graph using the Louvain Community Detection\n","    Algorithm.\n","\n","    References\n","    [1] Blondel, V.D. et al. Fast unfolding of communities in large networks.\n","        J. Stat. Mech 10008, 1-12(2008).\n","        https://doi.org/10.1088/1742-5468/2008/10/P10008\n","    [2] Traag, V.A., Waltman, L. & van Eck, N.J. From Louvain to Leiden:\n","        guaranteeing well-connected communities. Sci Rep 9, 5233 (2019).\n","        https://doi.org/10.1038/s41598-019-41695-z\n","    [3] Nicolas Dugué, Anthony Perez. Directed Louvain : maximizing modularity\n","        in directed networks. [Research Report] Université d’Orléans. 2015.\n","        hal-01231784. https://hal.archives-ouvertes.fr/hal-01231784\n","    \"\"\"\n","\n","    # initialize random seed\n","    np.random.seed(seed)\n","    # normalize matrix - otherwise it doesn't work -  read by rows\n","    A = sps.csr_matrix(A)/A.sum()\n","    # initialize the community assignment matrix to \"each node is a community\"\n","    C = sps.csr_matrix(sps.identity(A.shape[0], dtype='float'))\n","    # starting assignment\n","    if (C_start==None):\n","      C_start = sps.csr_matrix(sps.identity(A.shape[0], dtype='float'))\n","\n","    # main loop for the different layers of Louvain\n","    while True:\n","        print([C.shape[0], C.shape[1]])\n","        # improve modularity in this layer\n","        Clayer, improvement = _my_one_level(A,C_start)\n","        # exit if no improvement\n","        if (improvement==False): break\n","        # otherwise update variables according to the new clusters\n","        C = C*Clayer\n","        A = Clayer.T*(A*Clayer)\n","        # initialize the community assignment matrix to \"each node is a community\"\n","        C_start = sps.csr_matrix(sps.identity(A.shape[0], dtype='float'))\n","\n","    # return community assignments and resulting adjacency matrix\n","    Q = modularity_function(A)\n","    return C, A, Q\n","\n","print(\"softlouvain v1.10\")\n","\n","def _my_one_level(A,C):\n","\n","    N = A.shape[0] # number of nodes\n","    rand_nodes = list(range(N)) # random nodes list\n","    shuffle(rand_nodes) # shuffle random nodes list\n","\n","    d_in = np.transpose(A.sum(axis=1)) # input degrees - row vector\n","    d_out = A.sum(axis=0) # output degrees - row vector\n","    A = A + A.T # sum easily accessible by row\n","\n","    # main loop - loop until you do not see any improvement\n","    improvement = False\n","    while True:\n","\n","        # counter for the number of nodes changing community\n","        nb_moves = 0\n","        # test each node\n","        for i in rand_nodes:\n","            # get the community of node i\n","            ci_old = C[i,0:C.shape[1]].toarray()[0]\n","            # modify C for our purposes, i.e., exclude node i\n","            C[i,0:C.shape[1]] = 0\n","            # build vector v for evaluating modularity increase\n","            v = (A[i]-d_out[0,i]*d_in-d_in[0,i]*d_out)/2\n","            # find the maximum - best community\n","            ci = soft_assign(v[0,i],np.array(v*C)[0])\n","            # update matrix\n","            C[i,0:C.shape[1]] = ci\n","            # update counter (if needed)\n","            nb_moves += np.linalg.norm(ci-ci_old)\n","\n","        print(nb_moves)\n","        # exit if no improvement\n","        if (nb_moves<1e-10): break\n","        # otherwise: remove empty communities\n","        C = C[:,np.unique(scipy.sparse.find(C)[1])]\n","        # set improvement and reshuffle nodes for next try\n","        improvement = True\n","        shuffle(rand_nodes)\n","\n","    # exit\n","    return C, improvement"],"metadata":{"id":"blRsRqt3ZPrY"},"id":"blRsRqt3ZPrY","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}